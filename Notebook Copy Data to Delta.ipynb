{"cells":[{"cell_type":"code","source":["spark.conf.set(\"spark.sql.parquet.vorder.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.enabled\", \"true\")\n","spark.conf.set(\"spark.microsoft.delta.optimizeWrite.binSize\", \"1073741824\")"],"outputs":[],"execution_count":null,"metadata":{},"id":"29f65b0e-e17c-4458-ad82-15050066b2af"},{"cell_type":"code","source":["#parameters for the source folder and the json list of child tables to loop through\n","#this is passed in by a Data Pipeline\n","source_folder = ''\n","source_table_list = ''\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"tags":["parameters"]},"id":"03b5dc8b-fd82-4281-b1be-320f6d96a6ba"},{"cell_type":"markdown","source":[],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a9c9aa22-0d35-4bfd-9a8f-f69b9a25e680"},{"cell_type":"code","source":["from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","import ast\n","import json\n","\n","###########################################################\n","#create function to save each source folder to a Delta table\n","def loadDataFromSource(source_folder, table_name):\n","\n","    #load raw data from Files\n","    df = spark.read.format(\"parquet\").load('Files/' + source_folder + '/' + table_name)\n","\n","    df = df.withColumn(\"RawLoadDateTime\", current_timestamp()).withColumn(\"RawSourceFolder\", lit(source_folder))\n","\n","    #append new metadata columns and save to Delta table\n","    df.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").save(\"Tables/\" + table_name)\n","###########################################################\n","\n","#load json list of tables\n","table_list = json.loads(source_table_list)\n","\n","#create new empty list\n","table_result_list = []\n","\n","#loop over json object and save name of table to list\n","for json_dict in table_list:\n","    table_result_list.append(json_dict['name'])\n","\n","for i in table_result_list:\n","    loadDataFromSource(source_folder,i)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"b413e534-bbb4-4a58-8730-96c91bc06e9b"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","display_name":"python"},"microsoft":{"host":{},"language":"python"},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"save_output":true,"spark_compute":{"compute_id":"/trident/default","session_options":{"enableDebugMode":false,"conf":{}}},"notebook_environment":{},"synapse_widget":{"version":"0.1","state":{}},"trident":{"lakehouse":{"known_lakehouses":[{"id":"e7291081-bc2f-4c3d-af2a-0ba62fe55613"}],"default_lakehouse":"e7291081-bc2f-4c3d-af2a-0ba62fe55613","default_lakehouse_name":"Learning_Lakehouse","default_lakehouse_workspace_id":"3f004dd0-f4b1-4f63-9920-6b6e7c16c026"}}},"nbformat":4,"nbformat_minor":5}